% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cargar_datos.R
\name{cargar_datos}
\alias{cargar_datos}
\title{Extract data and metadata from \url{https://datos.gob.es/}}
\usage{
cargar_datos(path_id, encoding = "UTF-8", ...)
}
\arguments{
\item{path_id}{The end path of a dataset such as 'l01280148-seguridad-ciudadana-actuaciones-de-seccion-del-menor-en-educacion-vial-20141'
from \url{https://datos.gob.es/es/catalogo/l01280148-seguridad-ciudadana-actuaciones-de-seccion-del-menor-en-educacion-vial-20141}.
Must be a character string of length 1.}

\item{encoding}{The encoding passed to read (all) the csv(s). Most cases should be resolved with either
'UTF-8', latin1' or 'ASCII'. There are edge cases such as when printing any of the dataframes in the
data slot results in the error 'input string 1 is invalid UTF-8'. When that happens, use
\code{\link[readr]{guess_encoding}} to determine the encoding and try reading the dataset with the
new encoding.}

\item{...}{Arguments passed to \code{\link[readr]{read_csv}} and the other related \code{read_*} functions.
Internally, \code{cargar_datos} determines the delimiter of the file being read but the arguments
for each of these functions are practically the same, so it doesn't matter how \code{cargar_datos}
determines the delimiter, any of the arguments will work on all \code{read_*} functions.}
}
\value{
if \code{path_id} is a valid dataset path, a list with two slots: metadata and data. If \code{path_id}
is not a valid dataset path, it returns an empty list. See the details section for some caveats.
}
\description{
Extract data and metadata from \url{https://datos.gob.es/}
}
\details{
\code{cargar_datos} can return two possible outcomes: either an empty list or a list with a slot called metadata
and another slot called data. Whenever the \code{path_id} argument is an invalid dataset path, it will return an empty list.
When \code{path_id} is a valid dataset path, \code{cargar_datos} will return an a list with the two slots described above.

For the metadata slot, \code{cargar_datos} returns a \code{\link[tibble]{tibble}} with most available metadata of the dataset.
The columns are:

\itemize{
\item keywords: the available keywords from the dataset in the homepage of the dataset.
\item language: the available languages of the dataset's metadata. Note that that this does not mean that the dataset
is in different languages but only the meta data.
\item description: a short description of the data being read.
\item url: the url of the dataset in \url{https://datos.gob.es/}. Note that this URL is not the access URL to the dataset
but to the dataset's homepage in \url{https://datos.gob.es/}.
\item date_issued: the date at which the dataset was uploaded.
\item date_modified: the date at which the last dataset was uploaded. If the dataset has only been uploaded once, this
will return \code{'No modification date available'}.
\item publisher: the entity that publishes the dataset. See \code{datos_publisher} for all available publishers.
\item publisher_data_url: the homepage of the dataset in the website of the publisher. This is helpful to look
at the definitions of the columns in the dataset.
}

The metadata of the API can sometimes be returned in an incorrect order. For example, there are cases when there are several
languages available and the order of the different descriptions are not in the same order of the languages. If you
find any of these errors, try raising the issue directly to \url{https://datos.gob.es/} as the package extracts all
metadata in the same order as it is.

Whenever the metadata is in different languages, the resulting \code{\link[tibble]{tibble}} will have
the same numer of rows as there are languages containing the different texts in different languages and
repeating the same information whenever it's similar across languages (such as the dates, which are language agnostic).

In case the API returns empty requests, both data and metadata will be empty \code{\link[tibble]{tibble}}'s
with the same column names.

For the data slot, \code{cargar_datos} returns a list containing at least one \code{\link[tibble]{tibble}}.
If the dataset being request has file formats that \code{cargar_datos} can read (see \code{permitted_formats})
it will read those files. If that dataset has several files, then it will return a list of the same length
as there are datasets where each slot in that list is a \code{\link[tibble]{tibble}} with the data. If for
some reason any of the datasets being read cannot be read, \code{cargar_datos} has a fall back mechanism
that returns the format that attempted to read together with the URL so that the user can try to read the
dataset directly. In any case, the result will always be a list with \code{\link[tibble]{tibble}}'s
where each one could be the requested dataset (success) or a dataset with the format and url that attempted
to read but failed (failure).

The API of \url{https://datos.gob.es/} is not completely homogenous because it is an aggregator
of many different API's from different cities and provinces of Spain. \code{cargar_datos} can only read
a limited number of file formats but will keep increasing as the package evolves. You can check the available file formats
in \code{permitted_formats}. If the file format of the requested \code{path_id} is not readable, \code{cargar_datos}
will return only one data frame with all available formats with their respective data URL inside the data slot
so that users can read the manually.

In a similar line, in order for \code{cargar_datos} to provide the safest behaviour, it is very conservative in which
publisher it can read from \url{https://datos.gob.es/}. Because some publishers do not have standardized datasets
reading many different publishers can become very messy. \code{cargar_datos} currently reads files from selected
publishers because they offer standardized datasets which makes it safer to read. See the publishers that the
package can read in \code{publishers_available}.
}
\examples{

id <- 'l01080193-numero-total-de-edificios-con-viviendas-segun-numero-de-plantas'
some_data <- cargar_datos(id)

# Print the file to get some useful information
some_data

# Access the metadata
some_data$metadata

# Access the data
some_data$data

}
