} else { # Otherwise, sleep a second and try again
Sys.sleep(1)
waiting_GET(url, attempts_left - 1)
}
}
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
url
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
url
req <- waiting_GET(url, attempts_left)
req
#' Make GET requests with repeated trials
#'
#' @param url A url, preferably from the \code{path_*} functions
#' @param attempts_left Number of attempts of trying to request from the website
get_resp <- function(url, attempts_left = 5) {
stopifnot(attempts_left > 0)
resp <- httr::GET(url)
# Ensure that returned response is application/json
if (httr::http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
# On a successful GET, return the response
if (httr::status_code(resp) == 200) {
resp
} else if (attempts_left == 1) { # When attempts run out, stop with an error
stop_for_status(resp) # Return appropiate error message
} else { # Otherwise, sleep a second and try again
Sys.sleep(1)
get_resp_GET(url, attempts_left - 1)
}
}
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
url
req <- get_resp(url)
req
content(req)
httr::content(req)
data_list <- httr::content(req)
data_list
parsed_response <- httr::content(req)
parsed_response$result$items
c(data_list, list(data_lists))
data_list <- NULL
data_lists <- NULL
data_list <- parsed_response$result$items
data_lists <- c(data_list, list(data_lists))
data_lists
data_list <- NULL
data_lists <- list()
data_list <- parsed_response$result$items
data_lists <- c(data_list, list(data_lists))
data_lists
parsed_response$result$next
parsed_response$result$next`
parsed_response$result$next
parsed_response$result
page <- 3
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
url
req <- get_resp(url)
parsed_response <- httr::content(req)
parsed_response$result$`next`
page 0
page <- 0
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
parsed_response$result$`next``
parsed_response$result$`next`
0 - 1
-1 > -1
0 > -1
c(list(), NULL)
c(list(), list(NULL))
data_list <- NULL
data_lists <- NULL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
data_lists
url
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
data_lists
data_list <- NULL
data_lists <- NULL
page = 0
num_pages = 3
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
data_lists
is.null(parsed_response$result$`next`)
page <- page + 1
num_pages <- num_pages - 1
page
num_pages
url
data_list <- NULL
data_lists <- NULL
num_pages > 0
data_list <- NULL
data_lists <- NULL
num_pages
num_pages <- 5
data_list <- NULL
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
data_lists
#' Make GET requests with repeated trials
#'
#' @param url A url, preferably from the \code{path_*} functions
#' @param attempts_left Number of attempts of trying to request from the website
get_resp <- function(url, attempts_left = 5, ...) {
stopifnot(attempts_left > 0)
resp <- httr::GET(url, ...)
# Ensure that returned response is application/json
if (httr::http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
# On a successful GET, return the response
if (httr::status_code(resp) == 200) {
resp
} else if (attempts_left == 1) { # When attempts run out, stop with an error
stop_for_status(resp) # Return appropiate error message
} else { # Otherwise, sleep a second and try again
Sys.sleep(1)
get_resp_GET(url, attempts_left - 1)
}
}
parse_paginated_resp <- function(url, page = 0, num_pages = 1, ...) {
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url, ...)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
# Return the last parsed_response but with
# all accumulated datasets in the items slot
parsed_response$result$items <- data_lists
parsed_response
}
parse_paginated_resp(path_publishers())
pt <- parse_paginated_resp(path_publishers())
pt$result$items
url <- path_publishers()
parse_paginated_resp <- function(url, num_pages = 1, page = 0, ...) {
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url, ...)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
# Return the last parsed_response but with
# all accumulated datasets in the items slot
parsed_response$result$items <- data_lists
parsed_response
}
pt <- parse_paginated_resp(path_publishers(), 5)
pt$result$items %>% length()
length(pt$result$items)
pt$result$items
pt <- parse_paginated_resp(path_publishers(), 10)
pt$result$items
xml2::read_html("http://datos.gob.es/recurso/sector-publico/org/Organismo")
xml2::read_html("http://datos.gob.es/recurso/sector-publico/org/Organismo") %>% rvest::html_table()
xml2::read_html("http://datos.gob.es/recurso/sector-publico/org/Organismo") %>% rvest::html_table()
library(magrittr)
xml2::read_html("http://datos.gob.es/recurso/sector-publico/org/Organismo") %>% rvest::html_table()
url
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
num_pages
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url, ...)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
data_lists
source("R/extract_components.R")
source("R/url_paths.R")
source("R/extract_components.R")
source("R/utils.R")
source("R/utils.R")
source("R/extract_components.R")
extract_url_format("l01100377-residencias-de-mayores-caceres")
extract_url_format("I01100377-residencias-de-mayores-caceres")
extract_url_format("i01100377-residencias-de-mayores-caceres")
url
url <- path_publishers()
url
num_pages = 5
page = 0
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url, ...)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
data_lists
parse_paginated_resp <- function(url, num_pages = 1, page = 0, ...) {
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
# Return the last parsed_response but with
# all accumulated datasets in the items slot
parsed_response$result$items <- data_lists
parsed_response
}
pt <- parse_paginated_resp(path_publishers(), 5)
pt
dplyr::transpose(pt$result$items)
purrr::transpose(pt$result$items)
pt <- purrr::transpose(pt$result$items)
pt$notation
unlist(pt$notation)
any(unlist(pt$notation) == "A12002994")
any(unlist(pt$notation) == "L01462444")
any(unlist(pt$notation) == "E00134403")
#' Make GET requests over several pages of a URL
#'
#' @param url URL to request from, preferably from the \code{path_*} functions
#' @param num_pages Number of pages to request
#' @param page The page at which the request should being. This should rarely be used
#' @param ... Arguments passed to \code{\link[httr]{GET}}
#'
#' @return the parsed JSON object as a list but inside the items
#' slots it contains all data lists obtained from the pages specified
#' in \code{num_pages}
parse_paginated_resp <- function(url, num_pages = 1, page = 0, ...) {
# For the parsed items data_list
data_list <- NULL
# For the accumulated number of data_lists
data_lists <- NULL
while (num_pages > 0) {
# IMPORTANT!!
# All number of page queries should be specified as arguments in this
# function rather than in the URL
url <- httr::modify_url(url, query = list("_pageSize" = 50, "_page" = page))
req <- get_resp(url)
parsed_response <- httr::content(req)
data_list <- parsed_response$result$items
data_lists <- c(data_list, data_lists)
if (is.null(parsed_response$result$`next`)) {
# finished pagination, can quit
break
}
# set up for next iteration
page <- page + 1
num_pages <- num_pages - 1
}
# Return the last parsed_response but with
# all accumulated datasets in the items slot
parsed_response$result$items <- data_lists
parsed_response
}
resp <- parse_paginated_resp(path_publishers(), 100)
list_tibbles <- lapply(resp$result$items, function(x) dplyr::as_tibble(x[-1]))
do.call(rbind, list_tibbles)
resp <- parse_paginated_resp(path_publishers(), 100)
list_tibbles <- lapply(resp$result$items, function(x) dplyr::as_tibble(x[-1]))
publisher_df <- bind_rows(list_tibbles)
names(publisher_df) <- c('publisher_code', 'publisher')
publisher_df <- dplyr::bind_rows(list_tibbles)
names(publisher_df) <- c('publisher_code', 'publisher')
publisher
resp <- parse_paginated_resp(path_publishers(), 100)
list_tibbles <- lapply(resp$result$items, function(x) dplyr::as_tibble(x[-1]))
publisher_df <- dplyr::bind_rows(list_tibbles)
names(publisher_df) <- c('publisher_code', 'publisher')
publisher
publisher_df
datos_publiser()
#' Request all available publishers from datos.gob.es
#'
#' @return a \code{\link[tibble]{tibble}} with two columns: publisher_code and publishers
#' @export
#'
#' @examples
#'
#' datos_publiser()
#'
datos_publisher <- function() {
resp <- parse_paginated_resp(path_publishers(), 100)
# Delete the publisher URL because they ALL lead to the same
# URL which is this one http://datos.gob.es/recurso/sector-publico/org/Organismo#data
list_tibbles <- lapply(resp$result$items, function(x) dplyr::as_tibble(x[-1]))
publisher_df <- dplyr::bind_rows(list_tibbles)
names(publisher_df) <- c('publisher_code', 'publisher')
publisher_df
}
datos_publisher()
all_publishers
all_publishers <- datos_publisher()
#' Translate publisher code to publisher name
#'
#' @param code A publisher code
translate_publisher <- function(code) {
all_publishers <- datos_publisher()
index <- which(all_publishers$publisher_code == code)
if (length(index) == 0) return("Publisher not available")
all_publishers$publisher[index]
}
id <- 'l01080193-numero-total-de-edificios-con-viviendas-segun-numero-de-plantas'
resp <- httr::content(get_resp(path_dataset_id(id)))
data_list <- resp$result$items[[1]]
if (!data_list_correct(data_list)) {
return(character())
}
if (!'publisher' %in% names(data_list)) {
"No publisher available"
}
data_list$publisher
data_list$publisher
data_list
sub(".*//", "", data_list$publisher)
extract_url_format
sub(".*////", "", data_list$publisher)
sub(".*\\/", "", data_list$publisher)
translate_publisher(publisher_code)
publisher_code <- sub(".*\\/", "", data_list$publisher)
translate_publisher(publisher_code)
source("R/utils.R")
source("R/extract_components.R")
source("R/url_paths.R")
source("R/extract_components.R")
id <- 'l01080193-numero-total-de-edificios-con-viviendas-segun-numero-de-plantas'
resp <- httr::content(get_resp(path_dataset_id(id)))
data_list <- resp$result$items[[1]]
extract_metadata(data_list)
#' Request all available publishers from datos.gob.es
#'
#' @return a \code{\link[tibble]{tibble}} with two columns: publisher_code and publishers
#' @export
#'
#' @examples
#'
#' datos_publiser()
#'
datos_publisher <- function() {
resp <- parse_paginated_resp(path_publishers(), 100)
# Delete the publisher URL because they ALL lead to the same
# URL which is this one http://datos.gob.es/recurso/sector-publico/org/Organismo#data
list_tibbles <- lapply(resp$result$items, function(x) dplyr::as_tibble(x[-1]))
publisher_df <- dplyr::bind_rows(list_tibbles)
names(publisher_df) <- c('publisher_code', 'publisher')
publisher_df
}
source("R/utils.R")
extract_metadata(data_list)
extract_url_format("i01100377-residencias-de-mayores-caceres")
data_list
extract_url_format(data_list)
?httr::content
usethis::create_package(".")
