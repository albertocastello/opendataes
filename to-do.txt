## Next versions
- Play around with the spatial and begin/end date end points to see what they do

- if the end path is a zip file, do we unzip and check for file end paths such as .csv, .xml, ...? It's not a terrible idea and easy to do. rio::import already takes care of this! If we read the arguments, import unzips and we can provide a format so that import reads that format inside the zip file. Test if first.

- Include XML format

- Try to fix the JSON problem of only returning lists

- Include more publishers. First big cities.

- We should add the Frequency at which the data is uploaded and place it in the metadata tibble. Apparently, this is in the data_list but it's wrong. data_list$accrualPeriodicity returns a link that should give the frequency at which the data changes but once you enter it's always a dead website. For example, http://datos.gob.es/catalogo/a16003011-estadistica-de-personal-al-servicio-de-la-administracion-de-la-comunidad-autonoma-de-euskadi-2017/Frequency. We should report this to datos.gob.es.

- First file to deal with when workin with Bilbao: http://datos.gob.es/es/catalogo/a16003011-estadistica-de-personal-al-servicio-de-la-administracion-de-la-comunidad-autonoma-de-euskadi-2017
##

## TO-DO's:

- Fix this:

id <- 'l01080193-descripcion-de-la-causalidad-de-los-accidentes-gestionados-por-la-guardia-urbana-en-la-ciudad-de-barcelona'
pl <- cargar_datos(id)

- Add exploring functions so that the user doesn't have to enter the website.

- TRY MANY DIFFERENT DATASETS TO CHECK WHETHER WE HAVE A WORKING PACKAGE

- Extend vignette

- Create and develop vignette of functionality

- Fix travis

- Increase code coverage

- Add tests for exploratory of data functions

- Assume that the pkg reads a dataset from an excel file but reads it incorrectly. That is, it confuses headers with rows and adds some empty rows because of the excel format. I think that in the metadata we should provide all the direct links to each type of format in case the user wants to read the data directly. This eliminates us from the loop because they stop using the package but it allows the user to fully customize how they read the data. Jorge Lopez

##

## To use as examples in docs:

- The encoding 'Latin-1' works well for some datasets but doesn't for others. For example, this .csv from http://datos.gob.es/catalogo/l01350167-apartamentos does not read it well. But using 'encoding' = 'UTF-8' works well. We should perhaps find a way to combined both. readr has a nice feature to detect encodings. ORRR, we can allow the user to provide the encoding and they can iteratively check which one works best. I think this might be the best approach.

- The title in the metadata is not always in the same order as the language and the description. The description I think is **always** in the same order as the language as in they comin in the same nested lsit inside data_list. The title, however, is a separate vector inside the list. For example, for this example:

id <- 'a16003011-indicadores-del-mercado-laboral-del-ano-2005-al-2014'
pt <- cargar_datos(id)
pt$metadata

Language and description are alright but the title is wrong between languages.

- We have a problem w/ json files. The thing is that depending on the format of the json (which varies between ayuntamientos), the result from import::rio can be a data frame, a list, among other things.

For example, http://datos.gob.es/catalogo/l01230601-asociaciones throws an error because
the returned object from rio is not a df but a list that contains dfs. For other cases, rio might return a data frame and this is not a safe behaviour.

However, this JSON url: fromJSON("https://datosabiertos.ayto-arganda.es/dataset/2710a060-7b20-4786-b068-5df1f8be36b4/resource/2d34a895-26ef-49ae-9cfa-7b707d435334/download/arqu-religiosa.json") from http://datos.gob.es/catalogo/l01280148-patrimonio-cultural-arquitectura-religiosa1 returns the correct dataframe.

##
